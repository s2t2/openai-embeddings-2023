# -*- coding: utf-8 -*-
"""Botometer Users Sample Embeddings (Tweets and Profiles) - 20230704

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15z_v2MwhhHxNRf9v8ldEao5a75VnTEPg

In this notebook we load a previously-pulled sample of users and their tweets, then request embeddings from OpenAI, and save the embeddings back to a new CSV file in drive.

## Setup

### BigQuery Service
"""

from google.colab import auth

# asks you to login
auth.authenticate_user()

from google.cloud import bigquery
from pandas import DataFrame

PROJECT_ID = "tweet-research-shared"

class BigQueryService():
    def __init__(self):
        self.client = bigquery.Client(project=PROJECT_ID)

    def execute_query(self, sql, verbose=True):
        if verbose == True:
            print(sql)
        job = self.client.query(sql)
        return job.result()

    def query_to_df(self, sql, verbose=True):
        """high-level wrapper to return a DataFrame"""
        results = self.execute_query(sql, verbose=verbose)
        records = [dict(row) for row in list(results)]
        df = DataFrame(records)
        return df

bq = BigQueryService()
print("PROJECT:", bq.client.project)

"""### Google Drive"""

import os
from google.colab import drive

drive.mount('/content/drive')
print(os.getcwd(), os.listdir(os.getcwd())) #> 'content', ['.config', 'drive', 'sample_data']

# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
DATA_DIR = '/content/drive/MyDrive/Research/DS Research Shared 2023/data/impeachment_2020'
print(DATA_DIR)
assert os.path.isdir(DATA_DIR)

"""### Helper Functions"""

def split_into_batches(my_list, batch_size=10_000):
    """Splits a list into evenly sized batches"""
    # h/t: https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks
    for i in range(0, len(my_list), batch_size):
        yield my_list[i : i + batch_size]

def dynamic_batches(texts, batch_char_limit=30_000):
    """Splits texts into batches, with specified max number of characters per batch.
        Batches may have different lengths.
    """
    batches = []

    batch = []
    batch_chars = 0
    for text in texts:
        text_chars = len(text)

        if (batch_chars + text_chars) <= batch_char_limit:
            # THERE IS ROOM TO ADD THIS TEXT TO THE BATCH
            batch.append(text)
            batch_chars += text_chars
        else:
            # NO ROOM IN THIS BATCH, START A NEW ONE:

            if text_chars > batch_char_limit:
                # CAP THE TEXT AT THE MAX BATCH LENGTH
                text = text[0:batch_char_limit-1]

            batches.append(batch)
            batch = [text]
            batch_chars = text_chars

    if batch:
        batches.append(batch)

    return batches

texts = [
    "Short and sweet",
    "Short short",
    "I like apples, but bananas are gross.",
    "This is a tweet about bananas",
    "Drink apple juice!",
]
texts_df = DataFrame({"text": texts})
texts_df["chars"] = texts_df["text"].str.len()
texts_df

list(split_into_batches(texts_df["text"].tolist(), batch_size=2))

dynamic_batches(texts_df["text"].tolist(), batch_char_limit=30)

"""### OpenAI API Service

+ https://github.com/openai/openai-python
  + https://platform.openai.com/account/api-keys
  + https://platform.openai.com/docs/introduction/key-concepts
  + https://platform.openai.com/docs/models/overview
  + https://platform.openai.com/docs/guides/embeddings/what-are-embeddings
  + https://platform.openai.com/docs/guides/embeddings/embedding-models

> We recommend using `text-embedding-ada-002` for nearly all
 (Embedding) use cases. It's better, cheaper, and simpler to use.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install openai

from getpass import getpass

OPENAI_API_KEY = getpass("Please provide your OpenAI API Key: ")
print("...", OPENAI_API_KEY[-4:])

import openai
from openai import Model, Embedding
from pandas import DataFrame
from time import sleep

openai.api_key = OPENAI_API_KEY

MODEL_ID = "text-embedding-ada-002"

class OpenAIService():
    def __init__(self, model_id=MODEL_ID):
        self.model_id = model_id

    def get_models(self):
        models = Model.list()
        #print(type(models)) #> openai.openai_object.OpenAIObject

        records = []
        for model in sorted(models.data, key=lambda m: m.id):
            #print(model.id, "...", model.owned_by, "...", model.parent, "...", model.object)
            model_info = model.to_dict()
            del model_info["permission"] # nested list
            #print(model_info)
            records.append(model_info)

        models_df = DataFrame(records)
        #models_df.to_csv("openai_models.csv")
        #models_df.sort_values(by=["id"])
        return models_df

    def get_embeddings(self, texts):
        """Pass in a list of strings. Returns a list of embeddings for each."""
        result = Embedding.create(input=texts, model=MODEL_ID) # API CALL
        #print(len(result["data"]))
        return [d["embedding"] for d in result["data"]]

    def get_embeddings_in_batches(self, texts, batch_size=250, sleep_seconds=60):
        """High level wrapper to work around RateLimitError:
                Rate limit reached for [MODEL] in [ORG] on tokens per min.
                Limit: 1_000_000 tokens / min.

            batch_size : Number of users to request per API call

            sleep : Wait for a minute before requesting the next batch

            Also beware InvalidRequestError:
                This model's maximum context length is 8191 tokens,
                however you requested X tokens (X in your prompt; 0 for the completion).
                Please reduce your prompt; or completion length.

            ... so we should make lots of smaller requests.
        """
        #embeddings = []
        #counter = 1
        #for texts_batch in split_into_batches(texts, batch_size=batch_size):
        #    print(counter, len(texts_batch))
        #    embeds_batch = self.get_embeddings(texts_batch) # API CALL
        #    embeddings += embeds_batch
        #    counter += 1
        #    sleep(sleep_seconds)
        #return embeddings

        #embeddings = []
        #counter = 1
        #for texts_batch in split_into_batches(texts, batch_size=batch_size):
        #    print(counter, len(texts_batch))
        #    try:
        #        embeds_batch = self.get_embeddings(texts_batch)  # API CALL
        #        embeddings += embeds_batch
        #    except openai.error.RateLimitError as err:
        #        print(f"Rate limit reached. Sleeping for {sleep_seconds} seconds.")
        #        sleep(sleep_seconds)
        #        continue
        #    counter += 1
        #return embeddings

        embeddings = []
        counter = 1
        for texts_batch in split_into_batches(texts, batch_size=batch_size):
            print(counter, len(texts_batch))
            # retry loop
            while True:
                try:
                    embeds_batch = self.get_embeddings(texts_batch)  # API CALL
                    embeddings += embeds_batch
                    break  # exit the retry loop and go to the next batch
                except openai.error.RateLimitError as err:
                    print(f"... Rate limit reached. Sleeping for {sleep_seconds} seconds.")
                    sleep(sleep_seconds)
                    # retry the same batch
                #except openai.error.InvalidRequestError as err:
                #    print("INVALID REQUEST", err)
            counter += 1
        return embeddings


    def get_embeddings_in_dynamic_batches(self, texts, batch_char_limit=30_000, sleep_seconds=60):
        """High level wrapper to work around API limitations

            RateLimitError:
                Rate limit reached for [MODEL] in [ORG] on tokens per min.
                Limit: 1_000_000 tokens / min.

            AND

            InvalidRequestError:
                This model's maximum context length is 8191 tokens,
                however you requested X tokens (X in your prompt; 0 for the completion).
                Please reduce your prompt; or completion length.

            Params:

                batch_char_limit : Number of max characters to request per API call. Should be less than around 32_000 based on API docs.

                sleep : Wait for a minute before requesting the next batch

        """
        embeddings = []
        counter = 1
        for texts_batch in dynamic_batches(texts, batch_char_limit=batch_char_limit):
            print(counter, len(texts_batch))
            # retry loop
            while True:
                try:
                    embeds_batch = self.get_embeddings(texts_batch)  # API CALL
                    embeddings += embeds_batch
                    break  # exit the retry loop and go to the next batch
                except openai.error.RateLimitError as err:
                    print(f"... Rate limit reached. Sleeping for {sleep_seconds} seconds.")
                    sleep(sleep_seconds)
                    # retry the same batch
            counter += 1
        return embeddings

ai = OpenAIService()

#models_df = ai.get_models()
#models_df.head()

#texts = [
#    "I like apples, but bananas are gross.",
#    "This is a tweet about bananas",
#    "Drink apple juice!",
#]
#embeddings = ai.get_embeddings(texts)
#print(len(embeddings))
#print(len(embeddings[0])) #> 1536

"""## Users Sample

### Summary

Summary of the user sample we previously pulled when cross-checking botometer scores:
"""

sql = f"""
    SELECT
        u.opinion_community, u.is_bot, u.is_q
        ,count(distinct bom.user_id) as user_count
        , avg(cap) as avg_cap, avg(astroturf) as avg_astro
        --, avg(fake_follower) as avg_fakefollower, avg(financial) as avg_financial, avg(other) as avg_other
    FROM `tweet-research-shared.impeachment_2020.botometer_scores` bom
    JOIN `tweet-research-shared.impeachment_2020.user_details_v20210806_slim` u ON u.user_id = bom.user_id
    WHERE score_type='english' -- 7566
    GROUP BY 1,2,3
"""
print("SUMMARY OF BOTOMETER SAMPLE:")
bq.query_to_df(sql, verbose=False)

"""### Users

Fetch sample we already pulled when checking botometer scores, as well as their profiles (if they have them), as well as at most X of their tweets (pulled at random). The botometer scores table has multiple rows for some users, so we average the botometer scores for these users to arrive at one row per user in the sample.
"""

TWEET_MAX = 50
TWEET_DELIMETER = " " # " || "

sql = f"""
    SELECT
        u.user_id
        ,u.created_on
        ,u.screen_name_count
        ,u.screen_names
        ,u.status_count
        ,u.rt_count
        ,(u.rt_count / u.status_count) as rt_pct
        ,u.avg_toxicity
        ,u.avg_fact_score

        ,u.opinion_community
        ,u.is_bot
        ,u.is_q

        ,up.descriptions as profile_descriptions

        -- here we are grabbing at max X of the user's tweets at random:
        ,string_agg(t.status_text, '{TWEET_DELIMETER}' ORDER BY rand() LIMIT {int(TWEET_MAX)}) as tweet_texts

        ,avg(bom.cap) as bom_cap
        ,avg(bom.astroturf) as bom_astroturf
        ,avg(bom.fake_follower) as bom_fake_follower
        ,avg(bom.financial) as bom_financial
        ,avg(bom.other) as bom_other

    FROM `tweet-research-shared.impeachment_2020.botometer_scores` bom
    JOIN `tweet-research-shared.impeachment_2020.user_details_v20210806_slim` u ON u.user_id = bom.user_id
    JOIN `tweet-research-shared.impeachment_2020.tweets_v2` t on t.user_id = u.user_id
    LEFT JOIN `tweet-research-shared.impeachment_2020.user_profiles_v2` up on up.user_id = u.user_id
    WHERE bom.score_type='english' -- 7566
    GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13
    -- LIMIT 10
"""
print(f"USERS SAMPLE AND THEIR TWEETS (MAX {TWEET_MAX}):")
df = bq.query_to_df(sql, verbose=False)
#df.index = df["user_id"]
print(len(df))
df.head()

len(df[df["user_id"].duplicated()]["user_id"].unique()) #> 0 row per unique user in the sample

#def remove_delimeters(txt, delimeter=TWEET_DELIMETER):
#    return txt.replace(delimeter, " ")
#
# remove delimeters inserted during the data export process:
# df["tweet_texts"] = df["tweet_texts"].apply(remove_delimeters).tolist()

#users_df["profile_descriptions"].tolist()[0:5]

df["tweet_texts"].tolist()[0]

"""Should we remove special characters?

### Tweets
"""

tweets_df = df[df["tweet_texts"].notnull()][["user_id", "tweet_texts"]]

tweets_df["tweet_chars"] = tweets_df["tweet_texts"].str.len()
tweets_df["tweet_tokens"] = tweets_df["tweet_chars"] / 4
tweets_df.head()

tweets_df["tweet_chars"].describe()

import plotly.express as px

px.violin(tweets_df, x="tweet_chars", orientation="h",box=True, title="Distribution of User Tweet Text Lengths", height=350)

"""Distribution is bi-modal. Less tweet text for humans than bots. There are some outliers with much longer tweet texts. We may need to cap their text at a reasonable number of characters.

We need to cap the text length at the max number of characters allowable for an OpenAI API request.

> 1 token ~= 4 chars in English.

 https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them


But this token to character ratio is inexact, so we may need to further decrease in practice.
"""

max_tokens_per_request = 8_000 # 8191
max_characters_per_request = max_tokens_per_request * 4 # around four chars per token but this is inexact
print("MAX CHARS PER REQUEST:", max_characters_per_request) #> 32_000

#tweets_df["tweet_texts"].str[0:8]

TWEET_CHARS_MAX = 10_000 # 32_0000

tweets_df["tweet_texts"] = tweets_df["tweet_texts"].str[0:TWEET_CHARS_MAX]
tweets_df["tweet_chars"] = tweets_df["tweet_texts"].str.len()
tweets_df["tweet_tokens"] = tweets_df["tweet_chars"] / 4

px.violin(tweets_df, x="tweet_chars", orientation="h",box=True, title="Distribution of User Tweet Text Lengths", height=350)

# https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
# 1 token ~= 4 chars in English.
#tokens_limit = 1_000_000 # per minute
#characters_limit = tokens_limit * 4
#
#characters_total = tweets_df["tweet_chars"].sum()
#batches_needed = characters_total / characters_limit
#batch_size_needed = len(tweets_df) / batches_needed
#
#print("BATCHES NEEDED:", batches_needed)
#print("EST. BATCH SIZE:", batch_size_needed)

"""### Profiles

Not all users have profile texts. The API doesn't like null values or empty strings, so we will need to remove these rows before processing. And outer join them back together with the tweet embeddings at the end.
"""

#users_df["profile_descriptions"].isna()
#users_df["profile_descriptions"].notnull()

profiles_df = df[df["profile_descriptions"].notnull()][["user_id", "profile_descriptions"]]
print(len(profiles_df))

# filter out '' values
profiles_df = profiles_df[profiles_df["profile_descriptions"].str.strip() != ""]
print(len(profiles_df))
#profiles.head()

"""## Embeddings

### Tweet Embeddings
"""

tweet_texts = tweets_df["tweet_texts"].tolist()
print(len(tweet_texts))

tweet_embeddings = ai.get_embeddings_in_dynamic_batches(
    tweet_texts,
    batch_char_limit=15_000
)
print(len(tweet_embeddings))

tweets_df["embeddings"] = tweet_embeddings



"""### Profile Embeddings"""

profile_texts = profiles_df["profile_descriptions"].tolist()
print("PROFILE TEXTS:", len(profile_texts))
print(profile_texts[0:5])

profile_embeddings = ai.get_embeddings_in_dynamic_batches(
    profile_texts,
    batch_char_limit=15_000
)
print(len(profile_embeddings))

profiles_df["embeddings"] = profile_embeddings

#profiles_df["embeddings"].iloc[0]

"""## Save Embeddings"""

embeds_df = df.merge(profiles_df["embeddings"], left_index=True, right_index=True, how="outer") # outer join to keep users who don't have profiles
embeds_df.rename(columns={"embeddings": "profile_embeddings"}, inplace=True)

embeds_df = embeds_df.merge(tweets_df["embeddings"], left_index=True, right_index=True, how="outer")
embeds_df.rename(columns={"embeddings": "tweet_embeddings"}, inplace=True)

#embeds_df.head()

model_dirpath = os.path.join(DATA_DIR, MODEL_ID)
os.makedirs(model_dirpath, exist_ok=True)

embeddings_csv_filepath = os.path.join(model_dirpath, "botometer_sample_openai_embeddings_20230704.csv")
print(embeddings_csv_filepath)
embeds_df.to_csv(embeddings_csv_filepath)