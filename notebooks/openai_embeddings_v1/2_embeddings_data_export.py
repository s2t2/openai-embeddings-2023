# -*- coding: utf-8 -*-
"""2 - Botometer Users Sample and OpenAI Embeddings - Data Export

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-IdthxJSZbz0-7h9cYQTvs0VSfQSC4ti

For a sample of users for which we have previously obtained Botometer scores, we have obtained OpenAI embeddings of their tweet texts and profile descriptions, respectively.

The file with all the embeddings is large, and the embeddings are in a JSON string format, so instead let's split the embeddings into a column per embedding. And split the single large file into two smaller files (one for tweets, one for profiles).

The resulting files will be easier to use for analysis.

### Google Drive
"""

import os
from google.colab import drive

drive.mount('/content/drive')
print(os.getcwd(), os.listdir(os.getcwd())) #> 'content', ['.config', 'drive', 'sample_data']

# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
DATA_DIR = '/content/drive/MyDrive/Research/DS Research Shared 2023/data/impeachment_2020'
print(DATA_DIR)
assert os.path.isdir(DATA_DIR)

"""### Load Data"""

MODEL_ID = "text-embedding-ada-002"

embeddings_csv_filepath = os.path.join(DATA_DIR, MODEL_ID, "botometer_sample_openai_embeddings_20230704.csv")
assert os.path.isfile(embeddings_csv_filepath)

from pandas import read_csv

df = read_csv(embeddings_csv_filepath)
df.drop(columns=["Unnamed: 0"], inplace=True)
#df.index = df["user_id"]
print(df.columns.tolist())
df.head()

print(len(df))
print(df["tweet_embeddings"].notna().sum())
print(df["profile_embeddings"].notna().sum())

"""### Group Labels"""

df["opinion_label"] = df["opinion_community"].map({0:"Anti-Trump", 1:"Pro-Trump"})
df["bot_label"] = df["is_bot"].map({True:"Bot", False:"Human"})
df["q_label"] = df["is_q"].map({True:"Q-anon", False:"Normal"})

short_q_label = df["is_q"].map({True:" Q-anon ", False:" "})
df["group_label"] = df["opinion_label"] + short_q_label + df["bot_label"]
df["group_label"] = df["group_label"].replace(["Pro-Trump Q-anon Bot", "Pro-Trump Q-anon Human"], ["Q-anon Human", "Q-anon Bot"])
df["group_label"].value_counts()

"""### Separate Datasets"""

#LABELS = ['user_id', 'created_on',
#          'screen_name_count', 'screen_names', 'status_count', 'rt_count', 'rt_pct',
#          'avg_toxicity', 'avg_fact_score', 'opinion_community', 'is_bot', 'is_q',
#          'bom_cap', 'bom_astroturf', 'bom_fake_follower', 'bom_financial', 'bom_other'
#          #'profile_descriptions', 'tweet_texts',
#]
#ENGINEERED_LABELS = ["opinion_label", "bot_label", "q_label", "group_label"] #, "group_color"
#TWEET_LABELS = LABELS + ENGINEERED_LABELS + ["tweet_texts"]
#PROFILE_LABELS = LABELS + ENGINEERED_LABELS + ["profile_descriptions"]
#
#tweets_df = df[TWEET_LABELS] #.merge(tweet_embeddings, left_index=True, right_index=True)
#profiles_df = df[PROFILE_LABELS] #.merge(profile_embeddings, left_index=True, right_index=True)
#
#print(len(tweets_df.columns))
#print(len(profiles_df.columns))

tweets_df = df.copy()
tweets_df.drop(columns=["profile_descriptions", "profile_embeddings"], inplace=True)
print(tweets_df.shape)

profiles_df = df.copy()
profiles_df.drop(columns=["tweet_texts", "tweet_embeddings"], inplace=True)
profiles_df = profiles_df[ profiles_df["profile_embeddings"].notna() ]  # drop rows where there are no profile descriptions (not all users have profiles)
profiles_df.reset_index(inplace=True, drop=True)
print(profiles_df.shape)

"""### Unpack Embeddings

The embeddings are stored in a single column as a JSON string, so we'll need to convert that single column into a column per value in the embeddings array. We'll get 1536 columns back.
"""

import json

def unpack(embeddings_str):
    # idempotence check
    if isinstance(embeddings_str, str):
        return json.loads(embeddings_str)
    else:
        return embeddings_str

tweets_df["tweet_embeddings"] = tweets_df["tweet_embeddings"].apply(unpack)
profiles_df["profile_embeddings"] = profiles_df["profile_embeddings"].apply(unpack)

print(type(tweets_df["tweet_embeddings"][0]))
print(len(tweets_df["tweet_embeddings"][0])) #> 1536

print(type(profiles_df["profile_embeddings"][0]))
print(len(profiles_df["profile_embeddings"][0])) #> 1536

from pandas import DataFrame

tweet_embeddings = DataFrame(tweets_df["tweet_embeddings"].values.tolist())
print(len(tweet_embeddings))

profile_embeddings = DataFrame(profiles_df["profile_embeddings"].values.tolist())
print(len(profile_embeddings))

"""Merge embedding columns with label columns:"""

tweets_df = tweets_df.drop(columns=["tweet_embeddings"]).merge(tweet_embeddings, left_index=True, right_index=True)
profiles_df = profiles_df.drop(columns=["profile_embeddings"]).merge(profile_embeddings, left_index=True, right_index=True)

print(tweets_df.shape)
print(profiles_df.shape)

"""### Export Data"""

# export and download from google colab filesystem (file won't download from drive for some reason - maybe too big?)
# https://cmdlinetips.com/2020/05/how-to-save-pandas-dataframe-as-gzip-zip-file/
#df.to_csv("botometer_sample_openai_embeddings_20230704.csv.zip", index=False, compression="zip")

csv_filename = "botometer_sample_openai_tweet_embeddings_20230704.csv.gz"
csv_filepath = os.path.join(DATA_DIR, MODEL_ID, csv_filename)

#tweets_df.to_csv(csv_filename, index=False, compression="gzip")
tweets_df.to_csv(csv_filepath, index=False, compression="gzip")

csv_filename = "botometer_sample_openai_profile_embeddings_20230704.csv.gz"
csv_filepath = os.path.join(DATA_DIR, MODEL_ID, csv_filename)

#profiles_df.to_csv(csv_filename, index=False, compression="gzip")
profiles_df.to_csv(csv_filepath, index=False, compression="gzip")