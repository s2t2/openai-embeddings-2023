# -*- coding: utf-8 -*-
"""Exporting Embeddings to Drive - 2020240128

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tFWFj1yUgGxS-8WveeSnpEdrsgiG4-jh

We fetched OpenAI embeddings and stored on BQ. Let's download a CSV file to drive for further analysis.

## Setup

### Google Drive
"""

import os
from google.colab import drive

drive.mount('/content/drive')
print(os.getcwd(), os.listdir(os.getcwd()))



# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
#DIRPATH = '/content/drive/MyDrive/Research/Disinfo Research Shared 2022'
#DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2023'
DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2024'

print(DIRPATH)
os.path.isdir(DIRPATH)

DATA_DIRPATH = os.path.join(DIRPATH, "projects", "Impeachment 2020 Embeddings", "data")
os.path.isdir(DATA_DIRPATH)

#csv_filepath = os.path.join("")
#print(os.path.isfile(users_csv_filepath))

"""### BigQuery Service"""

from google.colab import auth

# asks you to login
auth.authenticate_user()

from google.cloud import bigquery
from pandas import DataFrame, read_gbq


PROJECT_ID = "tweet-collector-py"

class BigQueryService():
    def __init__(self, project_id=PROJECT_ID):
        self.project_id = project_id
        self.client = bigquery.Client(project=self.project_id)

    def execute_query(self, sql, verbose=True):
        if verbose == True:
            print(sql)
        job = self.client.query(sql)
        return job.result()

    #def query_to_df(self, sql, verbose=True):
    #    """high-level wrapper to return a DataFrame"""
    #    results = self.execute_query(sql, verbose=verbose)
    #    return DataFrame([dict(row) for row in results])

    def query_to_df(self, sql, verbose=True):
        """high-level wrapper to return a DataFrame"""
        if verbose == True:
            print(sql)
        # https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html#pandas-read-gbq
        return read_gbq(sql, project_id=self.project_id) # progress_bar_type="tqdm_notebook"

bq = BigQueryService()
print(bq)

print("DATASETS:")
datasets = list(bq.client.list_datasets())
for ds in datasets:
    #print("...", ds.project, ds.dataset_id)
    print("...", ds.reference)

"""# Embeddings"""

DATASET_ADDRESS = "tweet-collector-py.impeachment_production"

sql = f"""
    SELECT
        count(distinct s.user_id) as user_count
        ,count(distinct s.status_id) as status_count
    FROM `{DATASET_ADDRESS}.botometer_sample` s
    JOIN `{DATASET_ADDRESS}.botometer_sample_max_50_openai_status_embeddings` emb
        ON s.status_id = emb.status_id
"""
bq.query_to_df(sql, verbose=False)

"""## User Embeddings

7566 users
"""

##%%time

sql = f"""
     SELECT
        u.user_id, u.created_on
        --, u.screen_name_count, u.screen_names, split(u.screen_names, ",")[0] as screen_name
        ,u.status_count, u.rt_count
        ,u.is_bot --, u.bot_rt_network
        ,u.opinion_community --, u.avg_score_lr, avg_score_nb, avg_score_bert
        , u.is_q --, u.q_status_count
        --, u.follower_count, u.follower_count_b, u.follower_count_h
        --, u.friend_count, u.friend_count_b, u.friend_count_h

        ,u.avg_toxicity --, u.avg_severe_toxicity, u.avg_insult, u.avg_obscene, u.avg_threat, u.avg_identity_hate
        , u.avg_fact_score -- ,u.fact_scored_count

        ,u.bom_astroturf, u.bom_overall --, u.bom_cap  --,u.bom_lookup_count
        --,u.bom_fake_follower, u.bom_financial, u.bom_other, u.bom_self_declared, u.bom_spammer

        ,emb.embeddings

    FROM `{DATASET_ADDRESS}.user_details_v20240128_slim` u
    JOIN `{DATASET_ADDRESS}.botometer_sample_max_50_openai_user_embeddings` emb
        ON emb.user_id = u.user_id
    -- LIMIT 10
"""

users_df = bq.query_to_df(sql, verbose=True)
print(users_df.shape)
users_df.head()

"""Saving CSV to drive:"""

user_embeddings_csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_user_embeddings.csv")
users_df.to_csv(user_embeddings_csv_filepath, index=False)

#import json
#
#def unpack(embeddings_str):
#    # idempotence check
#    if isinstance(embeddings_str, str):
#        return json.loads(embeddings_str)
#    else:
#        return embeddings_str
#
#embeds = df["embeddings"].apply(unpack)
#print(type(embeds))
#
#embeds = DataFrame(embeds.values.tolist())
#embeds.columns = [f"openai_{col}" for col in embeds.columns]
#embeds.index = df.index
#print(embeds.shape)
#embeds.head()

#df_embeds = df.drop(columns=["embeddings"]).merge(embeds, left_index=True, right_index=True)
#df_embeds.head()

#df.to_csv("status_embeddings.csv", index=False)



"""## Tweet Embeddings

183K statuses

Wow wow wow this is taking a long time (1hr +...) to stream the data down over the network...
"""

sql = f"""
    SELECT s.user_id, s.status_id, s.status_text, s.created_at, emb.embeddings
    FROM `{DATASET_ADDRESS}.botometer_sample` s
    JOIN `{DATASET_ADDRESS}.botometer_sample_max_50_openai_status_embeddings` emb
        ON s.status_id = emb.status_id
    -- LIMIT 10000
"""

tweets_df = bq.query_to_df(sql, verbose=True)
print(tweets_df.shape)
tweets_df.head()

tweets_df.shape

"""Saving CSV to drive:"""

status_embeddings_csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings.csv")
tweets_df.to_csv(status_embeddings_csv_filepath, index=False)







##from pandas import concat
##
##limit = 1_000
##offset = 0
##
##all = DataFrame()
##
##while offset < 5_500:
##    sql = f"""
##        SELECT s.user_id, s.status_id, s.status_text, s.created_at, emb.embeddings
##        FROM `{DATASET_ADDRESS}.botometer_sample` s
##        JOIN `{DATASET_ADDRESS}.botometer_sample_max_50_openai_status_embeddings` emb
##            ON s.status_id = emb.status_id
##        LIMIT {int(limit)}
##        OFFSET {int(offset)}
##    """
##
##    batch = bq.query_to_df(sql, verbose=True)
##    print(tweets_df.shape)
##    if batch.empty:
##        print("ALL DONE!")
##        break
##
##    concat(all, batch)
##    offset += limit