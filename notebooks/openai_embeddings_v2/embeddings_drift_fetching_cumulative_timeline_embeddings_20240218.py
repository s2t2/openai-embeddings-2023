# -*- coding: utf-8 -*-
"""Embeddings Drift - Fetching Cumulative Timeline Embeddings - 20240218

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13BvoJDRaw--k9qWFaqATaKon6YVqDIGh

Using "High RAM" runtime in Colab. Crashes with basic runtime.

## Setup

### Google Drive
"""

import os
from google.colab import drive

drive.mount('/content/drive')

# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2024'
os.path.isdir(DIRPATH)

DATA_DIRPATH = os.path.join(DIRPATH, "projects", "Impeachment 2020 Embeddings", "data")
os.path.isdir(DATA_DIRPATH)

"""### BigQuery Service"""

from google.colab import auth

# asks you to login
auth.authenticate_user()

# SOURCE: https://github.com/s2t2/openai-embeddings-2023/blob/main/app/bq_service.py

from google.cloud import bigquery
from pandas import DataFrame, read_gbq
#from datetime import datetime

PROJECT_ID = "tweet-collector-py"

class BigQueryService():
    def __init__(self, project_id=PROJECT_ID):
        self.project_id = project_id
        self.client = bigquery.Client(project=self.project_id)

    def execute_query(self, sql, verbose=True):
        if verbose == True:
            print(sql)
        job = self.client.query(sql)
        return job.result()

    #def query_to_df(self, sql, verbose=True):
    #    """high-level wrapper to return a DataFrame"""
    #    results = self.execute_query(sql, verbose=verbose)
    #    return DataFrame([dict(row) for row in results])

    def query_to_df(self, sql, verbose=True):
        """high-level wrapper to return a DataFrame"""
        if verbose == True:
            print(sql)
        # https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html#pandas-read-gbq
        #return read_gbq(sql, project_id=self.project_id) # progress_bar_type="tqdm_notebook"
        #progress_bar_type="tqdm_notebook"
        return read_gbq(sql, project_id=self.project_id, progress_bar_type="tqdm_notebook")

    # WRITING

    @staticmethod
    def split_into_batches(my_list, batch_size=10_000):
        """Splits a list into evenly sized batches"""
        # h/t: https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks
        for i in range(0, len(my_list), batch_size):
            yield my_list[i : i + batch_size]

    # @ staticmethod
    #def generate_timestamp(dt=None):
    #    """Formats datetime object for storing in BigQuery. Uses current time by default. """
    #    dt = dt or datetime.now()
    #    return dt.strftime("%Y-%m-%d %H:%M:%S")

    def insert_records_in_batches(self, table, records, batch_size=5_000):
        """
        Inserts records in batches because attempting to insert too many rows at once
            may result in google.api_core.exceptions.BadRequest: 400

        Params:
            table (table ID string, Table, or TableReference)
            records (list of dictionaries)
        """
        rows_to_insert = [list(d.values()) for d in records]
        #errors = self.client.insert_rows(table, rows_to_insert)
        errors = []
        batches = list(BigQueryService.split_into_batches(rows_to_insert, batch_size=batch_size))
        for batch in batches:
            errors += self.client.insert_rows(table, batch)
        return errors

bq = BigQueryService()
print(bq)

print("DATASETS:")
datasets = list(bq.client.list_datasets())
for ds in datasets:
    #print("...", ds.project, ds.dataset_id)
    if "user" not in str(ds.reference):
        print("...", ds.reference)

"""### OpenAI Service"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install openai==0.28 # ok so there is now a 1.0 interface but we originally obtained using earlier API, so pinning that here

from google.colab import userdata

OPENAI_API_KEY = userdata.get("OPENAI_API_KEY") #os.getenv("OPENAI_API_KEY")
print(OPENAI_API_KEY[0:3], "...")

# SOURCE: https://github.com/s2t2/openai-embeddings-2023/blob/main/app/openai_service.py

def split_into_batches(my_list, batch_size=10_000):
    """Splits a list into evenly sized batches"""
    # h/t: https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks
    for i in range(0, len(my_list), batch_size):
        yield my_list[i : i + batch_size]

def dynamic_batches(texts, batch_char_limit=30_000):
    """Splits texts into batches, with specified max number of characters per batch.
        Caps text length at the maximum batch size (individual text cannot exceed batch size).
        Batches may have different lengths.
    """
    batches = []

    batch = []
    batch_chars = 0
    for text in texts:
        text_chars = len(text)

        if (batch_chars + text_chars) <= batch_char_limit:
            # THERE IS ROOM TO ADD THIS TEXT TO THE BATCH
            batch.append(text)
            batch_chars += text_chars
        else:
            # NO ROOM IN THIS BATCH, START A NEW ONE:

            if text_chars > batch_char_limit:
                # CAP THE TEXT AT THE MAX BATCH LENGTH
                text = text[0:batch_char_limit-1]

            batches.append(batch)
            batch = [text]
            batch_chars = text_chars

    if batch:
        batches.append(batch)

    return batches

# SOURCE: https://github.com/s2t2/openai-embeddings-2023/blob/main/app/openai_service.py

import os
from time import sleep
from pprint import pprint
import json

import openai
from openai import Model, Embedding
from pandas import DataFrame
#from dotenv import load_dotenv

#load_dotenv()

MODEL_ID = "text-embedding-ada-002" #  os.getenv("OPENAI_EMBEDDING_MODEL_ID", default="text-embedding-ada-002")

openai.api_key = OPENAI_API_KEY

class OpenAIService():
    """OpenAI API Service

        + https://github.com/openai/openai-python
        + https://platform.openai.com/account/api-keys
        + https://platform.openai.com/docs/introduction/key-concepts
        + https://platform.openai.com/docs/models/overview
        + https://platform.openai.com/docs/guides/embeddings/what-are-embeddings
        + https://platform.openai.com/docs/guides/embeddings/embedding-models

        > 2023: "We recommend using `text-embedding-ada-002` for nearly all
        (Embedding) use cases. It's better, cheaper, and simpler to use."

        > 2024: New "large" and "small" models (TODO)
    """

    def __init__(self, model_id=MODEL_ID):
        self.model_id = model_id
        print("EMBEDDING MODEL:", self.model_id)


    def get_models(self):
        models = Model.list()
        #print(type(models)) #> openai.openai_object.OpenAIObject

        records = []
        for model in sorted(models.data, key=lambda m: m.id):
            #print(model.id, "...", model.owned_by, "...", model.parent, "...", model.object)
            model_info = model.to_dict()
            try:
                del model_info["permission"] # nested list
            except:
                pass
            #print(model_info)
            records.append(model_info)

        models_df = DataFrame(records)
        #models_df.to_csv("openai_models.csv")
        #models_df.sort_values(by=["id"])
        return models_df

    def get_embeddings(self, texts):
        """Pass in a list of strings. Returns a list of embeddings for each."""
        result = Embedding.create(input=texts, model=self.model_id) # API CALL
        #print(len(result["data"]))
        return [d["embedding"] for d in result["data"]]

    def get_embeddings_in_batches(self, texts, batch_size=250, sleep_seconds=60):
        """High level wrapper to work around RateLimitError:
                Rate limit reached for [MODEL] in [ORG] on tokens per min.
                Limit: 1_000_000 tokens / min.

            batch_size : Number of users to request per API call

            sleep : Wait for a minute before requesting the next batch

            Also beware InvalidRequestError:
                This model's maximum context length is 8191 tokens,
                however you requested X tokens (X in your prompt; 0 for the completion).
                Please reduce your prompt; or completion length.

            ... so we should make lots of smaller requests.
        """
        embeddings = []
        counter = 1
        for texts_batch in split_into_batches(texts, batch_size=batch_size):
            print(counter, len(texts_batch))
            # retry loop
            while True:
                try:
                    embeds_batch = self.get_embeddings(texts_batch)  # API CALL
                    embeddings += embeds_batch
                    break  # exit the retry loop and go to the next batch
                except openai.error.RateLimitError as err:
                    print(f"... Rate limit reached. Sleeping for {sleep_seconds} seconds.")
                    sleep(sleep_seconds)
                    # retry the same batch
                #except openai.error.InvalidRequestError as err:
                #    print("INVALID REQUEST", err)
            counter += 1
        return embeddings

    def get_embeddings_in_dynamic_batches(self, texts, batch_char_limit=30_000, sleep_seconds=60):
        """High level wrapper to work around API limitations

            RateLimitError:
                Rate limit reached for [MODEL] in [ORG] on tokens per min.
                Limit: 1_000_000 tokens / min.

            AND

            InvalidRequestError:
                This model's maximum context length is 8191 tokens,
                however you requested X tokens (X in your prompt; 0 for the completion).
                Please reduce your prompt; or completion length.

            Params:

                batch_char_limit : Number of max characters to request per API call.
                                    Should be less than around 32_000 based on API docs.

                sleep : Wait for a minute before requesting the next batch

        """
        embeddings = []
        counter = 1
        for texts_batch in dynamic_batches(texts, batch_char_limit=batch_char_limit):
            print("BATCH:", counter, "SIZE:", len(texts_batch))
            # retry loop
            while True:
                try:
                    embeds_batch = self.get_embeddings(texts_batch)  # API CALL
                    embeddings += embeds_batch
                    break  # exit the retry loop and go to the next batch
                except openai.error.RateLimitError as err:
                    print(f"... Rate limit reached. Sleeping for {sleep_seconds} seconds.")
                    sleep(sleep_seconds)
                    # retry the same batch
                except openai.error.ServiceUnavailableError as err:
                    print(f"... Service Unavailz. Sleeping for {sleep_seconds} seconds.")
                    print(err)
                    sleep(sleep_seconds)
                    # retry the same batch
            counter += 1
        return embeddings

ai = OpenAIService()

models_df = ai.get_models()
models_df.head()

#models_df["id"].tolist()

"""Text embeddings models:"""

from pandas import to_datetime

models_df["created_date"] = to_datetime(models_df["created"], unit="s").dt.date

models_df[models_df["id"].str.contains("text-embedding")]

"""## Sample of Users and Tweets

Users sample, with max 50 of their tweets sampled at random. Same dataset we are using for user-level and status-level embeddings.

### Loading Data
"""

sql = f"""
    SELECT t.user_id, t.row_num, t.status_id, t.status_text, t.created_at
            ,u.is_bot, u.opinion_community --, u.avg_fact_score, u.avg_toxicity
    FROM `tweet-collector-py.impeachment_production.botometer_sample_max_50` t
    JOIN `tweet-collector-py.impeachment_production.user_details_v20240128_slim` u on u.user_id = t.user_id

    -- LIMIT 100
"""

df = bq.query_to_df(sql)
df.head()

print(len(df)) #> 183738
print(df["status_id"].nunique()) #> 183727

print(len(df))
df.drop_duplicates(subset="status_id", inplace=True)
print(len(df))

"""### Inspecting the Data"""

from pandas import to_datetime

df["created_at"] = to_datetime(df["created_at"])
df["created_on"] = df["created_at"].dt.date

df.sort_values(by=["user_id", "created_at"], inplace=True)
df.reset_index(drop=True, inplace=True) # order based on time

df["user_ts"] = df.groupby("user_id").cumcount() + 1

df.tail(10)

user_counts = df.groupby("user_id")["user_ts"].max()
user_counts.name = "user_status_count"
df = df.merge(user_counts, left_on="user_id", right_index=True, how="left")
df.tail(10)

import plotly.express as px

chart_df = df.groupby("user_id")["status_id"].nunique()
px.histogram(chart_df, title="Number of Tweets per User (sample max 50)")

#import plotly.express as px
#
#chart_df = df.groupby("user_id")["status_id"].cumcount()
#px.histogram(chart_df, title="Number of Users with at least X tweets in sample")

"""If we choose a minimum threshold to limit the sample based on those users who have a large number of tweets (anywhere over 2 or 3), we are going to be capturing mostly bots, but there are a good number of humans in here as well"""

px.histogram(df, x="user_ts", title="Users with at least X tweets",
             color="is_bot", color_discrete_map={True: "purple", False: "grey"},
             #labels={"user_ts": "Number of Tweets in Sample (max 50 per user)"}
)

chart_df = df[df["user_status_count"].between(1, 10, inclusive="both")]
print(len(chart_df))
print("USERS:", chart_df["user_id"].nunique())
print("TWEETS:", chart_df["status_id"].nunique())
print("TEXTS:", chart_df["status_text"].nunique())
print("----------")
print(chart_df.groupby("is_bot")["user_id"].nunique())

px.histogram(chart_df, x="user_ts", title="Users with max of X tweets",
             color="is_bot", color_discrete_map={True: "purple", False: "grey"},
             labels={"user_ts": "Number of Tweets in Sample (max 50 per user)"},
             text_auto=True
)

LIMIT = 20 # 50
chart_df = df[df["user_status_count"] >= LIMIT]
chart_df = chart_df[chart_df["user_ts"]==LIMIT]

print("USERS:", chart_df["user_id"].nunique())
print("TWEETS:", chart_df["status_id"].nunique())
print("TEXTS:", chart_df["status_text"].nunique())

print(chart_df.groupby("is_bot")["user_id"].nunique())

#px.histogram(chart_df, x="user_ts", title=f"Users with at least {LIMIT} tweets",
#             color="is_bot", color_discrete_map={True: "purple", False: "grey"},
#             labels={"user_ts": "Number of Tweets in Sample (max 50 per user)"},
#             text_auto=True
#)

print("TWEETS:", len(df))
print("TEXTS:", df["status_text"].nunique())

print("USERS:", df["user_id"].nunique())
#df.tail(10)

"""### User Time Series

We have to get embeddings for each user in a cumulative way. It doesn't make sense to get cumulative embeddings for a user that has only one tweet. We can look them up individually from what we've already collected. We care about chains of X>2 or more.

5289 users remain.

We will fetch embeddings for cumulative chains, essentially one for each tweet in the dataset (181,450).

Although with concern for cost, for users in the dataset that have 50 tweets, we will probably be able to see drift after 10 or 20 tweets.

If we make the number too small, we may not capture the entire period. Remember these are already a random sample of the tweets, so they are likely to be more dispersed over the entire time period.

Perhaps we can just get them all (to cover entire period).
"""

CUMULATIVE_MAX = 20
# 10: 43,441 tweets
# 15: 61,856 tweets
# 20: 79,662 tweets
# 25: 97,090 tweets
# 50: 181,450 tweets

ts = df[df["user_status_count"] >= 2] # 181_450 tweets
ts = ts[ts["user_ts"].between(1, CUMULATIVE_MAX, inclusive="both")]

ts = ts[["user_id", "user_status_count", "user_ts", "status_id", "status_text", "created_at", "is_bot", "opinion_community",]]
ts.reset_index(inplace=True, drop=True)

print("TWEETS:", ts["status_id"].nunique())
print("TEXTS:", ts["status_text"].nunique())
print("USERS:", ts["user_id"].nunique())

print(ts.groupby("is_bot")["user_id"].nunique())

ts.tail(10)

"""### User Cumulative Timelines

Let's calculate the cumulative timelines.
"""

ts[["user_id", "user_ts", "status_text"]].head()

# ts.groupby("user_id")["status_text"].cumsum()

ts["cumulative_text"] = ts.groupby("user_id", group_keys=False)["status_text"].apply(lambda txt: (txt + " ").cumsum().str.strip())
#ts[["user_id", "status_text", "cumulative_text"]].head()

ts["status_length"] = ts["status_text"].str.len()
ts["cumulative_length"] = ts["cumulative_text"].str.len()
ts[["user_id", "status_text", "status_length", "cumulative_text", "cumulative_length"]].head()

ts["cumulative_length"].describe()

px.violin(ts, x="cumulative_length", title="Cumulative Length of User Timelines",
        points="all", box=True,
        color="is_bot", color_discrete_map={True: "purple", False: "grey"},
)

px.violin(ts, x="status_length", title="Length of Status Texts",
        #points="all",
        box=True,
        color="is_bot", color_discrete_map={True: "purple", False: "grey"},
)

"""## Fetching Embeddings

We have been setting char limit at 15K, so since all users cumulative timeline lengths are under this, we should get complete results.
"""

texts = ts["cumulative_text"].tolist()
texts[0:3]

embeddings = ai.get_embeddings_in_dynamic_batches(texts, batch_char_limit=15_000)
print(len(embeddings))

ts["embeddings"] = embeddings

"""## Saving Embeddings

Save to CSV / Parquet on drive:
"""

pq_filepath = os.path.join(DATA_DIRPATH, f"botometer_sample_max_50_openai_cumulative_embeddings.parquet.gz")
ts[[
    "user_id", "user_status_count", "user_ts",
    "status_id", "status_text", "created_at",
    "cumulative_length", "cumulative_text",
    "embeddings"
]].to_parquet(pq_filepath, index=False, compression="gzip")

"""Save to BQ:"""

embeddings_table_name = f"tweet-collector-py.impeachment_production.botometer_sample_max_50_openai_cumulative_embeddings"
embeddings_table = bq.client.get_table(embeddings_table_name) # API call!
embeddings_table

records = ts[[
    "user_id", "user_status_count", "user_ts",
    "status_id", #"status_text", "created_at",
    "cumulative_length", "cumulative_text",
    "embeddings"
]].to_dict("records")

# running into google api issues with larger batches -
# there are so many embeddings for each row, so we lower the batch count substantially
bq.insert_records_in_batches(embeddings_table, records, batch_size=50)

sql = f"""
    SELECT
        count(distinct user_id) as user_count
        , count(distinct status_id)  as status_count
    FROM `{embeddings_table_name}`
    -- LIMIT 100
"""

bq.query_to_df(sql)

sql = f"""
    SELECT
        *
    FROM `{embeddings_table_name}`
    LIMIT 10
"""

bq.query_to_df(sql)