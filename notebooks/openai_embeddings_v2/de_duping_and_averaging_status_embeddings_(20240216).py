# -*- coding: utf-8 -*-
"""De-duping and Averaging Status Embeddings (20240216)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N-aRJ6GfO72QkOSWetqLeXYx9gq_NA8c

In this notebook, we prepare a clean (de-duped) version of the status embeddings. And we re-construct user embeddings using the average of their status embeddings.

This notebook saves both datasets back to drive for further analysis.

## Google Drive
"""

import os
from google.colab import drive

drive.mount('/content/drive')
print(os.getcwd(), os.listdir(os.getcwd()))

# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2024'

print(DIRPATH)
os.path.isdir(DIRPATH)

DATA_DIRPATH = os.path.join(DIRPATH, "projects", "Impeachment 2020 Embeddings", "data")
os.path.isdir(DATA_DIRPATH)

"""## Data Loading"""

from pandas import read_parquet

pq_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked.parquet.gzip")
statuses_df = read_parquet(pq_filepath)
print(statuses_df.shape)
print(statuses_df.columns)
statuses_df.head()

statuses_df["user_id"].nunique()

len(statuses_df)

statuses_df["status_id"].nunique()

"""Oh no, statuses not unique?"""

statuses_df["status_id"].value_counts()

statuses_df[statuses_df["status_id"].duplicated(keep=False)].sort_values("status_id")

"""The embeddings values appear to be the same for each status, so we can take the first row for each status.

## De-Duping

183,727 statuses
"""

print(statuses_df.shape)
statuses_df.drop_duplicates(subset=["status_id"], inplace=True)
print(statuses_df.shape)

"""Saving to drive:"""

pq_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked_deduped.parquet.gzip")

statuses_df.to_parquet(pq_filepath, compression="gzip")

"""## Averaging Embeddings per User"""

statuses_df.groupby("user_id")["status_id"].count()

embeddings_cols = [col for col in statuses_df.columns if "openai" in col]
print(len(embeddings_cols))
print(embeddings_cols[0], "...", embeddings_cols[-1])

averages = statuses_df.groupby("user_id")[embeddings_cols].mean()
print(averages.shape)
averages.head()

"""Get user labels from CSV file:"""

from pandas import read_csv

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_user_embeddings_unpacked.csv.gz")
users_df = read_csv(csv_filepath, compression="gzip")
print(users_df.shape)
print(users_df.columns)
users_df.head()

user_labels = users_df.drop(columns=embeddings_cols)
user_labels.index = user_labels["user_id"]
user_labels.head()

"""Merge user labels columns back in:"""

averages = averages.merge(user_labels, left_index=True, right_index=True)
averages.head()

"""Saving to drive:"""

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked_deduped_averaged.csv.gz")

averages.to_csv(csv_filepath, compression="gzip")