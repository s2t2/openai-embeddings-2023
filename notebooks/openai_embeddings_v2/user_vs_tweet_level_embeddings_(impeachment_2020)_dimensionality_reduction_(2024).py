# -*- coding: utf-8 -*-
"""User vs Tweet Level Embeddings (Impeachment 2020) - Dimensionality Reduction (2024)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UiL5SUTIm5V7_B6lf1ueFH97EHn9w6P0

We fetched user-level and tweet-level OpenAI embeddings and stored on BQ, and copied the data to CSV files on Drive.

Then we de-duped the status embeddings and calculated the average status embeddings for each user, and saved these CSV files on drive.

This notebook provides a preliminary analysis of user-level vs tweet-level embeddings, focusing first on dimensionality reduction.

## Setup

Package installation:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -U kaleido

"""May need to restart session before continuing."""

!pip list | grep kaleido

"""## Google Drive"""

import os
from google.colab import drive

drive.mount('/content/drive')
print(os.getcwd(), os.listdir(os.getcwd()))

# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
#DIRPATH = '/content/drive/MyDrive/Research/Disinfo Research Shared 2022'
#DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2023'
DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2024'

print(DIRPATH)
os.path.isdir(DIRPATH)

"""New project-based directory structure for 2024:

https://drive.google.com/drive/folders/1SuXkqVT400uZ2OYFGGV8SYBf7NhtBo5k?usp=drive_link
"""

DATA_DIRPATH = os.path.join(DIRPATH, "projects", "Impeachment 2020 Embeddings", "data")
os.path.isdir(DATA_DIRPATH)

os.listdir(DATA_DIRPATH)

"""The "unpacked" versions have a column per embedding, and are generally easier to work with.

The files we will be working with are:
  + "botometer_sample_max_50_openai_user_embeddings_unpacked.csv.gz" (user level embeddings) and
  +  "botometer_sample_max_50_openai_status_embeddings_v3_unpacked_deduped_averaged.csv.gz" (average status-level embeddings).
"""

RESULTS_DIRPATH = os.path.join(DIRPATH, "projects", "Impeachment 2020 Embeddings", "results")
os.makedirs(RESULTS_DIRPATH, exist_ok=True)
os.path.isdir(RESULTS_DIRPATH)

"""## Colors"""

# https://github.com/s2t2/openai-embeddings-2023/blob/main/app/colors.py

#GREY = "#ccc"
#PURPLE = "#7E57C2"

# colorbrewer scales
# light --> dark
BLUES = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#08519c', '#08306b']
REDS = ['#fff5f0', '#fee0d2', '#fcbba1', '#fc9272', '#fb6a4a', '#ef3b2c', '#cb181d', '#a50f15', '#67000d']
PURPLES = ['#fcfbfd', '#efedf5', '#dadaeb', '#bcbddc', '#9e9ac8', '#807dba', '#6a51a3', '#54278f', '#3f007d']
GREYS = ['#ffffff', '#f0f0f0', '#d9d9d9', '#bdbdbd', '#969696', '#737373', '#525252', '#252525', '#000000']
GREENS = ["#edf8e9","#c7e9c0","#a1d99b","#74c476","#41ab5d","#238b45","#005a32"]
ORANGES = ['#fff5eb', '#fee6ce', '#fdd0a2', '#fdae6b', '#fd8d3c', '#f16913', '#d94801', '#a63603', '#7f2704']
BROWNS = ["#C46200", "#964B00"]
RD_PU = ["#feebe2","#fcc5c0","#fa9fb5","#f768a1","#dd3497","#ae017e","#7a0177"]
PU_RD = ["#f1eef6","#d4b9da","#c994c7","#df65b0","#e7298a","#ce1256","#91003f"]

OPINION_COLORS_MAP = {"Anti-Trump": BLUES[5], "Pro-Trump": REDS[5]}
BOT_COLORS_MAP = {"Human": GREYS[3], "Bot": PURPLES[6]}
Q_COLORS_MAP = {"Normal": GREYS[3], "Q-anon": REDS[6]}
TOXIC_COLORS_MAP = {"Toxic": BROWNS[1], "Normal": GREYS[3]}
FACT_COLORS_MAP = {"High Quality": GREYS[3], "Low Quality": RD_PU[4]}

FOURWAY_COLORS_MAP = {
    "Anti-Trump Human": BLUES[3],
    "Anti-Trump Bot": BLUES[6],

    "Pro-Trump Human": REDS[3],
    "Pro-Trump Bot": REDS[6],
}
SIXWAY_COLORS_MAP = {
    "Anti-Trump Human": BLUES[3],
    "Anti-Trump Bot": BLUES[6],

    "Pro-Trump Human": REDS[3],
    "Pro-Trump Bot": REDS[6],

    "Q-anon Human": REDS[4], # "Pro-Trump Q-anon Human"
    "Q-anon Bot": REDS[7], # "Pro-Trump Q-anon Bot"
}


COLORS_MAP = {
    "bot_label": BOT_COLORS_MAP,
    "opinion_label": OPINION_COLORS_MAP,
    "q_label": Q_COLORS_MAP,
    "toxic_label": TOXIC_COLORS_MAP,
    "factual_label": FACT_COLORS_MAP,

    "fourway_label": FOURWAY_COLORS_MAP,
    "sixway_label": SIXWAY_COLORS_MAP,
    "bom_overall_label": BOT_COLORS_MAP,
    "bom_astroturf_label": BOT_COLORS_MAP,
}


BOT_LABEL_ORDER = ["Human", "Bot"]
CATEGORY_ORDERS = {
    "bot_label": BOT_LABEL_ORDER,
    "bom_overall_label": BOT_LABEL_ORDER,
    "bom_astroturf_label": BOT_LABEL_ORDER,
    "opinion_label": ["Anti-Trump", "Pro-Trump"],
    "q_label": ["Normal", "Q-anon"],

    "toxic_label": ["Normal", "Toxic"],
    "factual_label": ["High Quality", "Low Quality"],

    "fourway_label": list(FOURWAY_COLORS_MAP.keys()),
    "sixway_label": list(SIXWAY_COLORS_MAP.keys()),
}

"""## Dimensionality Reduction"""

#import warnings
#warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*") # suppress umap warnings https://github.com/slundberg/shap/issues/2909
#warnings.simplefilter("ignore", DeprecationWarning) # suppress warnings.warn("pkg_resources is deprecated as an API", DeprecationWarning) https://discuss.python.org/t/how-to-silence-pkg-resources-warnings/28629/7

import os

import numpy as np
from pandas import DataFrame
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

N_COMPONENTS = 2
#REDUCTION_RESULTS_DIRPATH = "results"
FIG_SHOW = True
FIG_SAVE = False

class ReductionPipeline:
    # adapted from: https://github.com/s2t2/openai-embeddings-2023/blob/main/app/reduction/pipeline.py

    def __init__(self, x, labels, target, n_components=N_COMPONENTS, reducer_type="PCA", #results_dirpath=None
                ):
        """

        """

        self.x = x.copy()
        self.labels = labels.copy()
        self.target = target

        self.reducer_type = reducer_type
        self.reducer_name = {"PCA": "pca", "T-SNE": "tsne", "UMAP": "umap"}[self.reducer_type]

        self.x_scaled = (self.x - self.x.mean(axis=0)) / self.x.std(axis=0)
        #scaler = StandardScaler()
        #self.x_scaled = scaler.fit_transform(self.x)

        self.n_components = n_components
        self.component_names = [f"component_{i+1}" for i in range(self.n_components)]

        #self.results_dirpath = results_dirpath or f"results_pca_{self.n_components}"
        #os.makedirs(self.results_dirpath, exist_ok=True)


    def perform(self):
        self.pca = PCA(n_components=self.n_components, random_state=99)
        print(self.pca)

        embeddings = self.pca.fit_transform(self.x_scaled)
        print("EMBEDDINGS:", embeddings.shape)
        self.embeddings_df = DataFrame(embeddings, columns=self.component_names, index=self.x.index)

        print("EXPLAINED VARIANCE RATIO:", self.pca.explained_variance_ratio_)
        print("EXPLAINED VARIANCE:", self.pca.explained_variance_ratio_.sum().round(2))

        # https://stackoverflow.com/questions/21217710/factor-loadings-using-sklearn/44728692#44728692
        loadings = self.pca.components_.T * np.sqrt(self.pca.explained_variance_)
        print("LOADINGS", loadings.shape)
        self.loadings_df = DataFrame(loadings, columns=self.component_names, index=self.pca.feature_names_in_)


    #def plot_embeddings(self, fig_show=True, fig_save=False, height=350, labels=None, hover_data=None):
    #
    #    labels = labels or self.labels
    #
    #    chart_df = self.embeddings_df.copy()
    #    chart_df = chart_df.merge(self.labels, left_index=True, right_index=True) # ADD TARGET BACK FOR COLOR (ASSUMES INDEX IS the SAME)
    #    #chart_df = chart_df.merge(self.x, left_index=True, right_index=True) # ADD aLL DATA BACK SO WE CAN INSPECT FEATURES AS WELL
    #    #chart_df.sort_values(by=self.target, inplace=True)
    #
    #    fig = None
    #    if self.n_components == 2:
    #        fig = px.scatter(chart_df, x="component_1", y="component_2",
    #            color=self.target, height=height,
    #            title="PCA Embeddings (n_components=2)",
    #            #hover_data=self.x.columns.tolist() #["gender", "island", "body_mass_g"]
    #            hover_data=hover_data
    #        )
    #    elif self.n_components == 3:
    #        fig = px.scatter_3d(chart_df, x="component_1", y="component_2", z="component_3",
    #            color=self.target, height=height,
    #            title="PCA Embeddings (n_components=3)",
    #            #hover_data=self.x.columns.tolist() # ["gender", "island", "body_mass_g"]
    #        )
    #
    #    if fig and fig_show:
    #        fig.show()
    #
    #    if fig and fig_save:
    #        html_filepath = os.path.join(self.results_filepath, f"features.html")
    #        fig.write_html(html_filepath)
    #
    #        png_filepath = os.path.join(self.results_filepath, f"features.png")
    #        fig.write_image(png_filepath)



    def plot_embeddings(self, height=500, fig_show=FIG_SHOW, fig_save=FIG_SAVE, results_dirpath=None,
                        subtitle=None, text=None, size=None, hover_data=None,
                        color=None, color_map=None, color_scale=None, category_orders=None):

        chart_df = self.embeddings_df.copy()
        chart_df = chart_df.merge(self.labels, left_index=True, right_index=True) # ADD TARGET BACK FOR COLOR (ASSUMES INDEX IS the SAME)
        #chart_df = chart_df.merge(self.x, left_index=True, right_index=True) # ADD aLL DATA BACK SO WE CAN INSPECT FEATURES AS WELL
        #chart_df.sort_values(by=self.target, inplace=True)

        title = f"Dimensionality Reduction Results ({self.reducer_type} n_components={self.n_components})"
        if subtitle:
            title += f"<br><sup>{subtitle}</sup>"

        chart_params = dict(x="component_1", y="component_2",
            title=title, height=height,
            #color=color, #"artist_name",
            hover_data= hover_data #{"index": (self.embeddings_df.index)} #hover_data #["audio_filename", "track_number"]
        )
        if color:
            chart_params["color"] = color
        if color_map:
            chart_params["color_discrete_map"] = color_map
        if color_scale:
            chart_params["color_continuous_scale"] = color_scale
        if category_orders:
            chart_params["category_orders"] = category_orders
        if hover_data:
            chart_params["hover_data"] = hover_data
        if size:
            chart_params["size"] = size
        if text:
            chart_params["text"] = text

        if self.n_components == 2:
            fig = px.scatter(chart_df, **chart_params)
        elif self.n_components == 3:
            chart_params["z"] = "component_3"
            fig = px.scatter_3d(chart_df, **chart_params)
        else:
            return None

        if fig_show:
            fig.show()

        if fig_save:
            results_dirpath = results_dirpath or self.results_dirpath
            filestem = os.path.join(results_dirpath, f"{self.reducer_name}_{self.n_components}")
            fig.write_image(f"{filestem}.png")
            fig.write_html(f"{filestem}.html")

        return fig

"""## User Embeddings

7566 users

### Loading

Loading CSV from drive:
"""

from pandas import read_csv

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_user_embeddings_unpacked.csv.gz")
users_df = read_csv(csv_filepath, compression="gzip")
print(users_df.shape)
print(users_df.columns)
users_df.head()

users_df["user_id"].nunique()

users_df["is_bot"].value_counts()

users_df["opinion_community"].value_counts()

users_df["avg_fact_score"].info()

from pandas import isnull

def add_labels(users_df):
    # APPLY SAME LABELS AS THE ORIGINAL SOURCE CODE
    # https://github.com/s2t2/openai-embeddings-2023/blob/1b8372dd36982009df5d4a80871f4c182ada743d/notebooks/2_embeddings_data_export.py#L51
    # https://github.com/s2t2/openai-embeddings-2023/blob/main/app/dataset.py#L37-L64

    # labels:
    users_df["opinion_label"] = users_df["opinion_community"].map({0:"Anti-Trump", 1:"Pro-Trump"})
    users_df["bot_label"] = users_df["is_bot"].map({True:"Bot", False:"Human"})
    users_df["fourway_label"] = users_df["opinion_label"] + " " + users_df["bot_label"]

    # language toxicity scores (0 low - 1 high)
    toxic_threshold = 0.1
    users_df["is_toxic"] = users_df["avg_toxicity"] >= toxic_threshold
    users_df["is_toxic"] = users_df["is_toxic"].map({True: 1, False :0 })
    users_df["toxic_label"] = users_df["is_toxic"].map({1: "Toxic", 0 :"Normal" })

    # fact check / media quality scores (1 low - 5 high)
    # there are null avg_fact_score, so we only apply operation if not null, and leave nulls
    fact_threshold = 3.0
    users_df["is_factual"] = users_df["avg_fact_score"].apply(lambda score: score if isnull(score) else score >= fact_threshold)
    users_df["is_factual"] = users_df["is_factual"].map({True: 1, False :0 })
    users_df["factual_label"] = users_df["is_factual"].map({1: "High Quality", 0 :"Low Quality" })

    # botometer binary and labels:
    users_df["is_bom_overall"] = users_df["bom_overall"].round()
    users_df["is_bom_astroturf"] = users_df["bom_astroturf"].round()
    users_df["bom_overall_label"] = users_df["is_bom_overall"].map({1:"Bot", 0:"Human"})
    users_df["bom_astroturf_label"] = users_df["is_bom_astroturf"].map({1:"Bot", 0:"Human"})
    users_df["bom_overall_fourway_label"] = users_df["opinion_label"] + " " + users_df["bom_overall_label"]
    users_df["bom_astroturf_fourway_label"] = users_df["opinion_label"] + " " + users_df["bom_astroturf_label"]

    return users_df


users_df = add_labels(users_df)
print(users_df.shape)
print(users_df.columns.tolist())
users_df.head()

users_df["is_factual"].value_counts()

users_df["factual_label"].value_counts()

users_df["is_toxic"].value_counts()

users_df["toxic_label"].value_counts()

users_df["bot_label"].value_counts()

users_df["opinion_label"].value_counts()

users_df["fourway_label"].value_counts()

"""### Splitting"""

users_df.index = users_df["user_id"]

embeddings_cols = [col for col in users_df.columns if "openai" in col]
print(len(embeddings_cols))
print(embeddings_cols[0], "...", embeddings_cols[-1])

users_x = users_df[embeddings_cols]
users_x.head()

#user_labels = users_df.drop(columns=embeddings_cols)
#print(user_labels.columns.tolist())
#user_labels.head()

"""### PCA 2"""

# /usr/local/lib/python3.10/dist-packages/plotly/express/_core.py:1223:
# PerformanceWarning: DataFrame is highly fragmented.
# This is usually the result of calling `frame.insert` many times, which has poor performance.
# Consider joining all columns at once using pd.concat(axis=1) instead.
# To get a de-fragmented frame, use `newframe = frame.copy()`
#  df_output[col_name] = to_unindexed_series(df_input[argument])

target = "fourway_label" #@param ["bot_label", "opinion_label", "fourway_label", "toxic_label", "is_factual"]
user_labels = users_df[target]
user_labels

users_pipeline = ReductionPipeline(x=users_x, labels=user_labels, target=target, n_components=2)

users_pipeline.perform()

users_pipeline.embeddings_df.head()



# todo: re-implement colors map and category orders
#users_pipeline.plot_embeddings(fig_show=False, fig_save=False, height=350, )

color_map = COLORS_MAP[target]
category_orders = {target: CATEGORY_ORDERS[target]}

users_pipeline.plot_embeddings(fig_show=False, fig_save=False, height=350,
    color=target, color_map=color_map, category_orders=category_orders
)

groupby_cols = [
    "bot_label", "opinion_label", # "bom_overall_label", "bom_astroturf_label",
    "toxic_label", "factual_label",
    "fourway_label", #"sixway_label",
]

for groupby_col in groupby_cols:
    color_map = COLORS_MAP[groupby_col]
    category_orders = {groupby_col: CATEGORY_ORDERS[groupby_col]}

    labels = users_df[groupby_col]
    pipeline = ReductionPipeline(x=users_x, labels=labels, target=groupby_col, n_components=2)

    results_dirpath = os.path.join(RESULTS_DIRPATH, "openai_embeddings_v2", "text-embedding-ada-002", f"user_embeddings_{pipeline.reducer_type.lower()}_{pipeline.n_components}", groupby_col)
    os.makedirs(results_dirpath, exist_ok=True)

    pipeline.perform()

    pipeline.plot_embeddings(
        color=groupby_col, color_map=color_map, category_orders=category_orders,
        #hover_data=["user_id", "bot_label"],
        fig_show=True, fig_save=True,
        results_dirpath=results_dirpath
    )

"""## Tweet Embeddings (User Averages)

183K statuses, averaged for each user (see prior notebook). 7566 rows resulting

### Loading
"""

from pandas import read_csv

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked_deduped_averaged.csv.gz")
averages_df = read_csv(csv_filepath)
print(averages_df.shape)
print(averages_df.columns)
averages_df.index = averages_df["user_id"]
averages_df.head()

averages_df["user_id"].nunique()

len(averages_df)

averages_df = add_labels(averages_df)
print(averages_df.shape)
print(averages_df.columns.tolist())
averages_df.head()

"""### Splitting"""

averages_x = averages_df[embeddings_cols]
averages_x

"""### PCA 2"""

averages_labels = averages_df[target]
averages_labels

averages_pipeline = ReductionPipeline(x=averages_x, labels=averages_labels, target=target, n_components=2)

averages_pipeline.perform()

averages_pipeline.embeddings_df.head()

# todo: re-implement colors map and category orders
#averages_pipeline.plot_embeddings(fig_show=True, fig_save=False, height=350)

groupby_cols = [
    "bot_label", "opinion_label", # "bom_overall_label", "bom_astroturf_label",
    "toxic_label", "factual_label",
    "fourway_label", #"sixway_label",
]

for groupby_col in groupby_cols:
    color_map = COLORS_MAP[groupby_col]
    category_orders = {groupby_col: CATEGORY_ORDERS[groupby_col]}

    labels = averages_df[groupby_col]
    pipeline = ReductionPipeline(x=averages_x, labels=labels, target=groupby_col, n_components=2)

    results_dirpath = os.path.join(RESULTS_DIRPATH, "openai_embeddings_v2", "text-embedding-ada-002", f"status_avg_embeddings_{pipeline.reducer_type.lower()}_{pipeline.n_components}", groupby_col)
    os.makedirs(results_dirpath, exist_ok=True)

    pipeline.perform()

    pipeline.plot_embeddings(
        color=groupby_col, color_map=color_map, category_orders=category_orders,
        #hover_data=["user_id", "bot_label"],
        fig_show=True, fig_save=True,
        results_dirpath=results_dirpath
    )