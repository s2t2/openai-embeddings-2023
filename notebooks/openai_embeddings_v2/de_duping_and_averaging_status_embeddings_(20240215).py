# -*- coding: utf-8 -*-
"""De-duping and Averaging Status Embeddings (20240225)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N-aRJ6GfO72QkOSWetqLeXYx9gq_NA8c

## Google Drive
"""

import os
from google.colab import drive

drive.mount('/content/drive')
print(os.getcwd(), os.listdir(os.getcwd()))

# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2024'

print(DIRPATH)
os.path.isdir(DIRPATH)

DATA_DIRPATH = os.path.join(DIRPATH, "projects", "Impeachment 2020 Embeddings", "data")
os.path.isdir(DATA_DIRPATH)

"""## Data Loading"""

from pandas import read_parquet

pq_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked.parquet.gzip")
statuses_df = read_parquet(pq_filepath)
print(statuses_df.shape)
print(statuses_df.columns)
statuses_df.head()

embeddings_cols = [col for col in statuses_df.columns if "openai" in col]
print(len(embeddings_cols))
print(embeddings_cols[0], "...", embeddings_cols[-1])

statuses_df["user_id"].nunique()

len(statuses_df)

statuses_df["status_id"].nunique()

"""Oh no, not unique?"""

statuses_df["status_id"].value_counts()

"""## De-Duping

183,727 statuses
"""

print(statuses_df.shape)
statuses_df.drop_duplicates(subset=["status_id"], inplace=True)
print(statuses_df.shape)

"""## Averaging Embeddings per User"""

statuses_df.groupby("user_id")["status_id"].count()

averages = statuses_df.groupby("user_id")[embeddings_cols].mean()
print(averages.shape)
averages.head()

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked_deduped_averaged.csv")

averages.to_csv(csv_filepath)