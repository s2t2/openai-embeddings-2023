# -*- coding: utf-8 -*-
"""Exporting Embeddings to Drive - 20240201 - v3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tFWFj1yUgGxS-8WveeSnpEdrsgiG4-jh

We fetched OpenAI embeddings and stored on BQ. Let's download a CSV file to drive for further analysis.

## Setup

### Google Drive
"""

import os
from google.colab import drive

drive.mount('/content/drive')
print(os.getcwd(), os.listdir(os.getcwd()))



# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
#DIRPATH = '/content/drive/MyDrive/Research/Disinfo Research Shared 2022'
#DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2023'
DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2024'

print(DIRPATH)
os.path.isdir(DIRPATH)

DATA_DIRPATH = os.path.join(DIRPATH, "projects", "Impeachment 2020 Embeddings", "data")
os.path.isdir(DATA_DIRPATH)

"""### BigQuery Service"""

from google.colab import auth

# asks you to login
auth.authenticate_user()

from google.cloud import bigquery
from pandas import DataFrame, read_gbq


PROJECT_ID = "tweet-collector-py"

class BigQueryService():
    def __init__(self, project_id=PROJECT_ID):
        self.project_id = project_id
        self.client = bigquery.Client(project=self.project_id)

    def execute_query(self, sql, verbose=True):
        if verbose == True:
            print(sql)
        job = self.client.query(sql)
        return job.result()

    #def query_to_df(self, sql, verbose=True):
    #    """high-level wrapper to return a DataFrame"""
    #    results = self.execute_query(sql, verbose=verbose)
    #    return DataFrame([dict(row) for row in results])

    def query_to_df(self, sql, verbose=True):
        """high-level wrapper to return a DataFrame"""
        if verbose == True:
            print(sql)
        # https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html#pandas-read-gbq
        #return read_gbq(sql, project_id=self.project_id) # progress_bar_type="tqdm_notebook"
        #progress_bar_type="tqdm_notebook"
        return read_gbq(sql, project_id=self.project_id, progress_bar_type="tqdm_notebook")

bq = BigQueryService()
print(bq)

print("DATASETS:")
datasets = list(bq.client.list_datasets())
for ds in datasets:
    #print("...", ds.project, ds.dataset_id)
    print("...", ds.reference)

"""## Helper Functions

### Unpacking Embeddings
"""

import json
from pandas import DataFrame


def unpack(embeddings_str):
    """Takes a string value containing an array of OpenAI embeddings,
        and returns a list of floats.
    """
    if isinstance(embeddings_str, str):
        return json.loads(embeddings_str)
    else:
        return embeddings_str


def unpacked(df, col_prefix="openai"):
    """Takes a dataframe witha single column of OpenAI embeddings,
        and unpacks them into their own separate columns,
        and returns a modified version of the original dataframe,
        with the original embeddings column replaced by the new unpacked columns
    """

    print("UNPACKING...")
    embeds = df["embeddings"].apply(unpack)
    print(type(embeds))

    print("RECONSTRUCTING...")
    embeds = DataFrame(embeds.values.tolist())
    embeds.columns = [f"{col_prefix}_{col}" for col in embeds.columns]
    embeds.index = df.index
    print(embeds.shape)
    #embeds.head()

    print("MERGING...")
    df_unpacked = df.merge(embeds, left_index=True, right_index=True)
    df_unpacked.drop(columns=["embeddings"], inplace=True)
    print(df_unpacked.shape)
    return df_unpacked

"""# Embeddings"""

DATASET_ADDRESS = "tweet-collector-py.impeachment_production"

sql = f"""
    SELECT
        count(distinct s.user_id) as user_count
        ,count(distinct s.status_id) as status_count
    FROM `{DATASET_ADDRESS}.botometer_sample` s
    JOIN `{DATASET_ADDRESS}.botometer_sample_max_50_openai_status_embeddings_v2` emb
        ON s.status_id = emb.status_id
"""
bq.query_to_df(sql, verbose=False)

"""## User Embeddings

7566 users
"""

sql = f"""
     SELECT
        u.user_id, u.created_on
        --, u.screen_name_count, u.screen_names, split(u.screen_names, ",")[0] as screen_name
        ,u.status_count, u.rt_count
        ,u.is_bot --, u.bot_rt_network
        ,u.opinion_community --, u.avg_score_lr, avg_score_nb, avg_score_bert
        , u.is_q --, u.q_status_count
        --, u.follower_count, u.follower_count_b, u.follower_count_h
        --, u.friend_count, u.friend_count_b, u.friend_count_h

        ,u.avg_toxicity --, u.avg_severe_toxicity, u.avg_insult, u.avg_obscene, u.avg_threat, u.avg_identity_hate
        , u.avg_fact_score -- ,u.fact_scored_count

        ,u.bom_astroturf, u.bom_overall --, u.bom_cap  --,u.bom_lookup_count
        --,u.bom_fake_follower, u.bom_financial, u.bom_other, u.bom_self_declared, u.bom_spammer

        ,emb.embeddings

    FROM `{DATASET_ADDRESS}.user_details_v20240128_slim` u
    JOIN `{DATASET_ADDRESS}.botometer_sample_max_50_openai_user_embeddings` emb
        ON emb.user_id = u.user_id
    -- LIMIT 10
"""

users_df = bq.query_to_df(sql, verbose=False)
print(users_df.shape)

users_df.head()

"""Saving CSV to drive:"""

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_user_embeddings.csv.gz")
users_df.to_csv(csv_filepath, index=False, compression="gzip")

"""### ... Unpacked"""

users_df_unpacked = unpacked(users_df)
print(users_df.shape)
users_df_unpacked.head()

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_user_embeddings_unpacked.csv.gz")
users_df_unpacked.to_csv(csv_filepath, index=False, compression="gzip")

"""## Tweet Embeddings

183K statuses

Wow wow wow this is taking a long time (1hr +...) to stream the data down over the network...

Re-doing with the statuses table v2, that has duplicate lookups removed (row per unique status)...

Re-doing with statuses table v3, which has status texts as well...
"""

sql = f"""
    SELECT user_id, status_id, status_text, created_at, embeds_length, embeddings
    FROM `{DATASET_ADDRESS}.botometer_sample_max_50_openai_status_embeddings_v3`
    -- LIMIT 10000
"""

tweets_df = bq.query_to_df(sql, verbose=True)
print(tweets_df.shape)
tweets_df.head()

tweets_df.head()

"""Saving CSV to drive:"""

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3.csv.gz")
tweets_df.to_csv(csv_filepath, index=False, compression="gzip")

"""### ... Unpacked"""

unpacked_tweets_df = unpacked(tweets_df)
unpacked_tweets_df.head()

# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html

pq_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked.parquet.gzip")
unpacked_tweets_df.to_parquet(pq_filepath, compression="gzip")

csv_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked.csv.gz")
unpacked_tweets_df.to_csv(csv_filepath, index=False, compression="gzip")

#arrow_filepath = os.path.join(DATA_DIRPATH, "botometer_sample_max_50_openai_status_embeddings_v3_unpacked.arrow")
#df.to_feather(arrow_filepath)

"""## Scratch Work"""

##from pandas import concat
##
##limit = 1_000
##offset = 0
##
##all = DataFrame()
##
##while offset < 5_500:
##    sql = f"""
##        SELECT s.user_id, s.status_id, s.status_text, s.created_at, emb.embeddings
##        FROM `{DATASET_ADDRESS}.botometer_sample` s
##        JOIN `{DATASET_ADDRESS}.botometer_sample_max_50_openai_status_embeddings` emb
##            ON s.status_id = emb.status_id
##        LIMIT {int(limit)}
##        OFFSET {int(offset)}
##    """
##
##    batch = bq.query_to_df(sql, verbose=True)
##    print(tweets_df.shape)
##    if batch.empty:
##        print("ALL DONE!")
##        break
##
##    concat(all, batch)
##    offset += limit

"""### Compressed Table

https://cloud.google.com/bigquery/docs/exporting-data#bigquery_extract_table_compressed-python
"""

# from google.cloud import bigquery
# client = bigquery.Client()
# bucket_name = 'my-bucket'

#destination_uri = "gs://{}/{}".format(bucket_name, "shakespeare.csv.gz")
#dataset_ref = bigquery.DatasetReference(project, dataset_id)
#table_ref = dataset_ref.table("shakespeare")
#job_config = bigquery.job.ExtractJobConfig()
#job_config.compression = bigquery.Compression.GZIP
#
#extract_job = client.extract_table(
#    table_ref,
#    destination_uri,
#    # Location must match that of the source table.
#    location="US",
#    job_config=job_config,
#)  # API request
#extract_job.result()  # Waits for job to complete.

# from google.cloud import bigquery
# client = bigquery.Client()
# bucket_name = 'my-bucket'


#from google.cloud import bigquery
#
#
##ds_ref = bigquery.DatasetReference(PROJECT_ID, DATASET_ADDRESS)
#DATASET_ID = "impeachment_production"
#ds_ref = bigquery.DatasetReference(PROJECT_ID, DATASET_ID)
#table_ref = ds_ref.table("botometer_sample_max_50_openai_status_embeddings_v3")
#
#job_config = bigquery.job.ExtractJobConfig()
#job_config.compression = bigquery.Compression.GZIP
#
#BUCKET_NAME = "impeachment-analysis-2020"
##destination_uri = f"gs://{BUCKET_NAME}/impeachment_production/botometer_sample_max_50_openai_status_embeddings_v4.csv.gz"
##> too large to be exported to a single file. Specify a uri including a * to shard export. See 'Exporting data into one or more files' in https://cloud.google.com/bigquery/docs/exporting-data.
#destination_uri = f"gs://{BUCKET_NAME}/impeachment_production/botometer_sample_max_50_openai_status_embeddings_v4_*.csv.gz"
#
#client = bq.client
#extract_job = client.extract_table(
#    table_ref,
#    destination_uri,
#    # Location must match that of the source table.
#    location="US",
#    job_config=job_config,
#)  # API request
#extract_job.result()  # Waits for job to complete.